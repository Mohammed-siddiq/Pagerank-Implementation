{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "Name: Mohammed Siddiq\n",
    "UID:  msiddi56@uic.edu\n",
    "UIN:  664750555\n",
    "'''\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "import re\n",
    "import itertools\n",
    "import operator\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "import numpy as np\n",
    "import string\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the file path containing the documents:\t/Users/mohammedsiddiq/Downloads/www\n",
      "Enter the window size:\t6\n"
     ]
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "file_path = input(\"Enter the file path containing the documents:\\t\")\n",
    "expected_pos = [\"NN\", \"NNS\", \"NNP\", \"NNPS\", \"JJ\"]\n",
    "custom_window_size = int(input(\"Enter the window size:\\t\"))\n",
    "counter = 0\n",
    "failed_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents(path):\n",
    "    df = pd.DataFrame()\n",
    "    abstract_path = path + \"//abstracts\"\n",
    "    print(\"loading abstract documents ...\")\n",
    "    print()\n",
    "    abstract_docs = os.listdir(abstract_path)\n",
    "    read_counter = 0\n",
    "    for doc in abstract_docs:\n",
    "        with open(abstract_path + \"//\" + doc, 'r') as content_file:\n",
    "            content = content_file.read()\n",
    "        #             print(content)\n",
    "        df = df.append({'abstract': content, 'docId': doc}, ignore_index=True)\n",
    "        read_counter += 1\n",
    "        # print(read_counter // len(abstract_docs) * 100, \"% done..\", flush=True)\n",
    "\n",
    "    # loading corresponding gold file\n",
    "    gold_path = path + \"//gold\"\n",
    "    gold_docs = os.listdir(gold_path)\n",
    "    print(\"loading gold documents ...\")\n",
    "    print()\n",
    "    read_counter = 0\n",
    "    for doc in gold_docs:\n",
    "        with open(gold_path + \"//\" + doc, 'r') as content_file:\n",
    "            content = content_file.read()\n",
    "            df.loc[df.docId == doc, 'gold'] = content\n",
    "        read_counter += 1\n",
    "        # print(read_counter // len(gold_docs) * 100, \"% done..\", flush=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "# df.shape\n",
    "\n",
    "\n",
    "# df.count()\n",
    "# df.loc[df['docId'] == '9466892']\n",
    "\n",
    "\n",
    "def remove_punctions(word):\n",
    "    for c in word:\n",
    "        if c in string.punctuation:\n",
    "            word = word.replace(c, '')\n",
    "    return word.strip()\n",
    "\n",
    "\n",
    "def preprocess(document):\n",
    "    #     print(\"Preprocessing abstract documents..\")\n",
    "    words = document.split()\n",
    "    processed_words = []\n",
    "    for word in words:\n",
    "        words_with_pos = word.split(\"_\")  # extract the Pos after _\n",
    "        #         words_with_pos = [remove_punctions(word) for word in words_with_pos]\n",
    "        if len(words_with_pos) == 2:\n",
    "            if len(words_with_pos[0]) > 1 and words_with_pos[0] not in stop_words and words_with_pos[1] in expected_pos:\n",
    "                final_word = ps.stem(words_with_pos[0])\n",
    "                processed_words.append(final_word)\n",
    "    return processed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_gold(document):\n",
    "    #     print(\"Preprocessing(Stemming) gold documents...\")\n",
    "    sentences = document.split(\"\\n\")\n",
    "    preprocessed_doc = []\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "        stemmed_sentence = ''\n",
    "        for word in words:\n",
    "            if len(word.strip()) > 0 and word not in stop_words:\n",
    "                final_word = ps.stem(word)\n",
    "                if len(stemmed_sentence) == 0:  # first word of the sentence\n",
    "                    stemmed_sentence += final_word\n",
    "                else:\n",
    "                    stemmed_sentence += ' ' + final_word\n",
    "        preprocessed_doc.append(stemmed_sentence)\n",
    "    return preprocessed_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_wighted_graph(document_words, window_size):\n",
    "    graph_rep = collections.defaultdict(lambda: collections.Counter())\n",
    "    for i, current_word in enumerate(document_words):\n",
    "        for windowI in range(i - window_size, i + window_size + 1):  # check in the window of the word\n",
    "            if windowI >= 0 and windowI < len(document_words):  # boundary checks for the window\n",
    "                if windowI != i:  # avoiding self match with the word\n",
    "                    window_word = document_words[windowI]\n",
    "                    graph_rep[current_word][\n",
    "                        window_word] += 1  # updating the counter if the current word co occurs with another word in the window\n",
    "    return graph_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_bag_of_words(graph_rep_df):\n",
    "    bow = set()\n",
    "    bow.update(list(graph_rep_df.columns.values))  # adding all columns\n",
    "    bow.update(list(graph_rep_df.index.values))  # adding all the rows\n",
    "    return bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_series(df, bow):\n",
    "    for word in bow:\n",
    "        if not word in df:\n",
    "            df[word] = pd.Series(0.0, index=df.index)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_matrix(df, bow):\n",
    "    df = df.copy()\n",
    "    df = fill_missing_series(df, bow)  # fill missing columns\n",
    "    df = fill_missing_series(df.T, bow).T  # fill missing rows\n",
    "    return df.fillna(0.0)  # replace Nan's with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_links_for_unliked(doc_matrix):\n",
    "    doc_matrix = doc_matrix.T  # transposing to get the columns\n",
    "    for column in doc_matrix:\n",
    "        if doc_matrix[column].sum() == 0.0:\n",
    "            doc_matrix[column] = pd.Series(np.ones(len(doc_matrix[column])), index=doc_matrix.index)\n",
    "    return doc_matrix.T "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_initial_probablity(word_nodes):\n",
    "    #     print(\"words \" , word_nodes)\n",
    "    initial_probability_val = 1.0 / float(len(word_nodes))\n",
    "    # creating a series representing initial probabilities for each word in the document\n",
    "    initial_probabilites = pd.Series({node: initial_probability_val for node in word_nodes})\n",
    "    return initial_probabilites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_values_as_probability(doc_matrix):\n",
    "    return doc_matrix.div(doc_matrix.sum(axis=1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_final_probability(word_nodes, alpha, initial_probabilities):\n",
    "    random_jump_probability = 1.0 / len(word_nodes) * (1 - alpha)\n",
    "    final_probability = initial_probabilities.copy().multiply(alpha) + random_jump_probability\n",
    "    return final_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_ngrams_and_score(document, ranked_words):\n",
    "    ranked_phrases = {}\n",
    "    # updating unigrams score\n",
    "    for word in document:\n",
    "        ranked_phrases[word] = ranked_words.get(word)\n",
    "    bigrams = list(nltk.ngrams(document, 2))\n",
    "    trigrams = list(nltk.ngrams(document, 3))\n",
    "\n",
    "    # updating bigrams score\n",
    "    for w1, w2 in bigrams:\n",
    "        ranked_phrases[w1 + \" \" + w2] = ranked_words.get(w1) + ranked_words.get(w2)\n",
    "    # updating trigrams score\n",
    "    for w1, w2, w3 in trigrams:\n",
    "        ranked_phrases[w1 + \" \" + w2 + \" \" + w3] = ranked_words.get(w1) + ranked_words.get(w2) + ranked_words.get(w3)\n",
    "    phrases_with_scroes = sorted(ranked_phrases.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    phrases = [phrase for phrase, rank in phrases_with_scroes]\n",
    "    rank = [rank for phrase, rank in phrases_with_scroes]\n",
    "    return phrases, rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "failed_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_page_rank(document_words, alpha, number_of_iterations, window_size, docId):\n",
    "    # build_wighted_graph\n",
    "    # print(\"building weighted graph for document : \" + docId)\n",
    "    global counter\n",
    "    global failed_counter\n",
    "\n",
    "    graph_ds = build_wighted_graph(document_words=document_words, window_size=window_size)\n",
    "    if len(graph_ds) != 0:\n",
    "        df_matrix = pd.DataFrame(graph_ds)\n",
    "\n",
    "        document_bow = extract_bag_of_words(df_matrix)\n",
    "\n",
    "        df_matrix = construct_matrix(df_matrix, document_bow)\n",
    "        df_matrix = create_links_for_unliked(df_matrix)\n",
    "\n",
    "        # print(\"Initialiazing matrix:\")\n",
    "        # print(df_matrix)\n",
    "\n",
    "        rank = set_initial_probablity(document_bow)\n",
    "        df_matrix_final = normalize_values_as_probability(df_matrix)\n",
    "        df_matrix_final = update_final_probability(word_nodes=document_bow, alpha=alpha,\n",
    "                                                   initial_probabilities=df_matrix_final)\n",
    "        # print(\"Calculating final rank\")\n",
    "        # print(df_matrix_final)\n",
    "        for i in range(number_of_iterations):\n",
    "            rank = rank.dot(df_matrix_final)\n",
    "\n",
    "        ranked_phrases, ranked_scores = form_ngrams_and_score(document=document_words, ranked_words=rank)\n",
    "        counter += 1\n",
    "        return ranked_phrases\n",
    "    else:\n",
    "        failed_counter += 1\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading abstract documents ...\n",
      "\n",
      "loading gold documents ...\n",
      "\n",
      "Done loading documents...\n",
      "                                            abstract     docId  \\\n",
      "0  A_DT large-scale_JJ study_NN of_IN the_DT evol...     62822   \n",
      "2  Structuring_VBG and_CC presenting_VBG annotate...   1848435   \n",
      "3  On_IN a_DT web_NN browsing_VBG support_NN syst...   8935942   \n",
      "4  Information_NN transfer_NN in_IN social_JJ med...  14376543   \n",
      "5  An_DT experimental_JJ study_NN of_IN large-sca...  13534613   \n",
      "\n",
      "                                                gold  \n",
      "0  degree of change\\nhypertext/hypermedia\\nmiscel...  \n",
      "2  document structure\\nrdf\\nsemantics\\nstyle\\nxht...  \n",
      "3  3d technology\\ngraphical user interfaces\\nvisu...  \n",
      "4  causality\\nentropy\\npoint processes\\npredictio...  \n",
      "5  betweenness centrality\\nclustering coefficient...  \n"
     ]
    }
   ],
   "source": [
    "df = load_documents(file_path)\n",
    "print(\"Done loading documents...\")\n",
    "df = df.dropna()\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing abstract documents...\n"
     ]
    }
   ],
   "source": [
    "print(\"Preprocessing abstract documents...\")\n",
    "df['abstract'] = df.abstract.apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            abstract     docId  \\\n",
      "0  [large-scal, studi, evolut, web, page, fast, w...     62822   \n",
      "2  [annot, media, repositori, hypermedia, present...   1848435   \n",
      "3  [web, support, system, 3d, visual, commerci, w...   8935942   \n",
      "4  [inform, transfer, social, media, recent, rese...  14376543   \n",
      "5  [experiment, studi, large-scal, mobil, social,...  13534613   \n",
      "\n",
      "                                                gold  \n",
      "0  degree of change\\nhypertext/hypermedia\\nmiscel...  \n",
      "2  document structure\\nrdf\\nsemantics\\nstyle\\nxht...  \n",
      "3  3d technology\\ngraphical user interfaces\\nvisu...  \n",
      "4  causality\\nentropy\\npoint processes\\npredictio...  \n",
      "5  betweenness centrality\\nclustering coefficient...  \n",
      "preprocessing [Stemming] gold documents....\n"
     ]
    }
   ],
   "source": [
    "print(df.head())\n",
    "print(\"preprocessing [Stemming] gold documents....\")\n",
    "df['gold'] = df.gold.apply(preprocess_gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "MRR_collection = [0.0 for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying page rank for individual documents...\n"
     ]
    }
   ],
   "source": [
    "print(\"Applying page rank for individual documents...\")\n",
    "df['ranked_phrases'] = df.abstract.apply(apply_page_rank, alpha=0.85, number_of_iterations=10, window_size=custom_window_size,\n",
    "                                         docId=df['docId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ranked phrases...\n",
      "0       [page checksum page, degre chang page, chang p...\n",
      "2       [structur document structur, repositori docume...\n",
      "3       [function web bookmark, function function user...\n",
      "4       [transfer entropi inform, entropi inform trans...\n",
      "5       [social network mobil, mobil social network, n...\n",
      "6       [rout search system, user rout search, interac...\n",
      "7       [world knowledg time, knowledg time space, yag...\n",
      "8       [human wayfind effici, human wayfind inform, i...\n",
      "9       [system author reput, author system author, lo...\n",
      "10      [approach web servic, web servic composit, com...\n",
      "11      [claim disput finder, disput claim user, user ...\n",
      "12      [interact cyberdrama gener, cyberdrama gener w...\n",
      "13      [retriev content-bas retriev, octopu retriev m...\n",
      "14      [co-brows solut co-brows, web page co-brows, c...\n",
      "15      [inform tourist site, tempor inform tourist, i...\n",
      "16      [set key algorithm, key algorithm descript, ur...\n",
      "17      [wikipedia articl concept, autopedia wikipedia...\n",
      "18      [xml schema xml, schema xml schema, access xml...\n",
      "19      [sempl semant web, semant portal semant, porta...\n",
      "20      [page chang user, page chang cam, resourc page...\n",
      "21      [minwis origin minwis, estim resembl b., unbia...\n",
      "22      [log queri log, search queri log, log web sear...\n",
      "23      [disput claim web, claim web pattern, extract ...\n",
      "24      [semant search task, search task ad-hoc, wod w...\n",
      "25      [index document index, document host index, in...\n",
      "26      [deep web dynabot, dynabot deep web, web dynab...\n",
      "27      [properti matchmak matchmak, matchmak matchmak...\n",
      "28      [web search engin, web crucial web, web applic...\n",
      "29      [social network inform, inform social network,...\n",
      "30      [conceptu ontology-bas open, ontology-bas open...\n",
      "                              ...                        \n",
      "1317    [IP anycast cdn, IP anycast algorithm, load-aw...\n",
      "1318    [web applic account, specif web applic, web ap...\n",
      "1319    [user servic algorithm, network user satisfact...\n",
      "1320    [experi real world, real world project, experi...\n",
      "1321    [request assist request, intent request assist...\n",
      "1322    [autom classif web, classif web page, web clas...\n",
      "1323    [javascript code javascript, malici javascript...\n",
      "1324    [soft pattern definit, pattern definit sentenc...\n",
      "1325    [differ languag topic, multilingu topic wikipe...\n",
      "1326    [data link data, web data content, aggreg web ...\n",
      "1327    [tag metadata pattern, web tag metadata, seman...\n",
      "1328    [annot proven log, proven log experi, web prov...\n",
      "1329    [traffic p2p traffic, p2p traffic p2p, traffic...\n",
      "1330    [search engin fresh, fresh metric search, fres...\n",
      "1331    [wrapper wrapper learn, asm learn system, lear...\n",
      "1332    [user related news, related news articl, news ...\n",
      "1333    [comput grid comput, comput cloud comput, comp...\n",
      "1334    [portlet famili portlet, famili portlet portle...\n",
      "1335    [web applic person, content web applic, person...\n",
      "1336    [challeng goal song, song dataset challeng, ch...\n",
      "1337    [proxi cooper leas, leas scalabl consist, leas...\n",
      "1338    [causal news event, futur news event, futur ev...\n",
      "1339    [document taxonomi hierarch, label document te...\n",
      "1340    [html document frame, html document document, ...\n",
      "1341    [servic select problem, optim select problem, ...\n",
      "1342    [bid marketplac bid, bid agent display, distri...\n",
      "1343    [inform spread context, spread context inform,...\n",
      "1344    [page cach page, school bangalor india, access...\n",
      "1345    [page imag imag, imag imag content, imag conte...\n",
      "1346    [dataset case-bas reason, case-bas reason effo...\n",
      "Name: ranked_phrases, Length: 1330, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"ranked phrases...\")\n",
    "print(df.ranked_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_MRR_document(ranked_phrases, MRR_collection, gold_phrases):\n",
    "    for top in range(10):\n",
    "        MRR_at_k = 0.0\n",
    "        for k in range(top):  # finding hits in the top k\n",
    "            if k < len(ranked_phrases) and ranked_phrases[k] in gold_phrases:\n",
    "                rank = gold_phrases.index(ranked_phrases[k]) + 1\n",
    "                MRR_at_k += 1.0 / rank\n",
    "                break;\n",
    "        MRR_collection[top] += MRR_at_k\n",
    "    return MRR_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating MRR\n"
     ]
    }
   ],
   "source": [
    "print(\"calculating MRR\")\n",
    "MRR_collection = [0.0 for i in range(10)]\n",
    "for ranked_phrases,gold in zip(df['ranked_phrases'],df['gold']):\n",
    "    MRR_collection = update_MRR_document(MRR_collection=MRR_collection,gold_phrases=gold,ranked_phrases=ranked_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "MRR_collection = [1/counter * mrr_at_k for mrr_at_k in MRR_collection]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " MRR @ k= 1 :  0.0\n",
      " MRR @ k= 2 :  0.004815650865312265\n",
      " MRR @ k= 3 :  0.008954100827689992\n",
      " MRR @ k= 4 :  0.011959592359378506\n",
      " MRR @ k= 5 :  0.016732246341964765\n",
      " MRR @ k= 6 :  0.021999364475900045\n",
      " MRR @ k= 7 :  0.0268212857199432\n",
      " MRR @ k= 8 :  0.030941820314518427\n",
      " MRR @ k= 9 :  0.037431092266544355\n",
      " MRR @ k= 10 :  0.0445237863795545\n"
     ]
    }
   ],
   "source": [
    "for k in range(10):\n",
    "    print(\" MRR @ k=\", k + 1, \": \", MRR_collection[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
